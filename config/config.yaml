defaults:
  - _self_
  - hydra: hydra_default

training_args:
  _target_: transformers.TrainingArguments

  do_train: true
  do_eval: true

  output_dir: ???

  # training steps
  num_train_epochs: 5
  max_steps: -1  # default -1; overrides "num_train_epochs"

  # evaluation
  evaluation_strategy: epoch  # steps
  eval_steps: 5000

  # batching
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1

  # lr
  learning_rate: 1e-4
  lr_scheduler_type: linear
  warmup_ratio: 0.1

  # regularization
  weight_decay: 0.01
  max_grad_norm: 1.0

  # saving
  save_strategy: ${training_args.evaluation_strategy}
  save_steps: ${training_args.eval_steps}
  save_total_limit: 3
  save_on_each_node: false

  # logging
  logging_strategy: steps
  logging_steps: 500

  # fp16
  fp16: true
  half_precision_backend: auto  # or cuda_amp
  fp16_opt_level: O1  # for apex
  fp16_full_eval: false

  # data pipeline
  dataloader_num_workers: 0
  dataloader_pin_memory: true

  # other
  no_cuda: false
  seed: 42
  disable_tqdm: false
  metric_for_best_model: mrr
  greater_is_better: true
  sharded_ddp: simple  # need fairscale (pip install fairscale)
  skip_memory_metrics: true
  label_smoothing_factor: 0.0
  ddp_find_unused_parameters: false

model:
  _target_: src.model.Decoder
  num_layers: 6
  num_heads: 8
  head_dim: 64
  dff: 2048
  dropout: 0.1
  max_length: 512  # for positional embeddings and relative attention
  padding_idx: 0
  num_special_tokens: 3  # pad, bos, unk
  use_pos_emb: false
  use_track_emb: true
  use_artist_emb: true
  num_tracks: null
  num_artists: null
  attention_type: "dot_product"  # {dot_product, relative}
  activation: "relu"  # {relu, gelu_new}
  emb_weighting_type: "average"  # {average, sigmoid}

training_dataset:
  _target_: ???
  data: null
  bos_id: 1
  unk_id: 2

valid_dataset:
  _target_: ???
  data: null
  bos_id: ${training_dataset.bos_id}
  unk_id: ${training_dataset.unk_id}

test_dataset:
  _target_: ???
  data: null
  bos_id: ${training_dataset.bos_id}
  unk_id: ${training_dataset.unk_id}

collator:
  _target_: ???
  padding_idx: ${model.padding_idx}

trainer_cls:
  _target_: hydra.utils.get_class
  path: ???

trainer_params:
  k: 100
  track_id_to_artist_id: null
  save_weights_only: true
  label_smoothing: 0.0

# other
train_data_path: ???
valid_data_path: ???
track_to_artist_path: ???
track_vocab_path: ???
artist_vocab_path: ???
limit: null